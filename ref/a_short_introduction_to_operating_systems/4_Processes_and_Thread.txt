Chapter 4

Processes and Thread


4.1 Key concepts

Multitasking and multi-user systems need to distinguish between the different
programs being executed by the system. This is accomplished with the concept
of a process.


4.1.1 Naming conventions

Before talking about process management we shall introduce some of the names
which are in common use. Not all operating systems or books agree on the
definitions of these names. In this chapter we shall take a liberal attritude
after all, it is the ideas rather than the names which count. Try to remember
the different terms - they will be used repeatedly.

  * Process: This is a general term for a program which is being executed.
    All work done by the CPU contributes to the execution of processes.
    Each process has a descriptive information structure associated with it
    (normally held by the kernel) called a process control block which keeps
    track of how far the execution has progressed and what resources the 
    process holds.

  * Task: On some systems processes are called tasks.

  * Job: Some systems distinguish between batch execution and interactive 
    execution. Batch (or queued) processes are often called jobs. They are
    like production line processes which start, do something and quit, without
    stopping to ask for input from a user. They are non-interactive processes.

  * Thread: (sometimes called a lightweight process) is different from process
    or task in that a thread is not enough to get a whole program executed.
    A thread is a kind of stripped down process - it is just one 'active hand' in
    a program - something which the CPU is doing on behalf of a program,
    but not enough to be called a complete process. Threads remember what
    they have done separately, but they share the information about what
    resources a program is uing, and what state the program is in. A thread
    is only a CPU assignment. Several threads can contribute to a single task.
    When this happens, the information about one process or task is used by
    many threads. Each task must have at least one thread in order to do 
    any work.

  * CPU burst: A period of uninterrupted CPU activity.

  * I/O burst: A period of uninterrupted input/output activity.


4.1.2 Scheduling

On most multitasking system, only one process can truly be active at at time -
the system must therefore share its time between the execution of many
processes. This sharing is called scheduling. (Scheduling <-> time management.)

Different methods of scheduling are appropriate for different kinds of execution.
A queue is one form of scheduling in which each program waits its trun
and is executed serially. This is not very useful for handling multitasking, but
it is necessary for scheduling devices which cannot be shared by nature. An
example of the latter is the printer. Each print job has to be completed before
the next one can begin, otherwise all the print jobs would be mixed up and
interleaved resulting in nonsense.

We shall make a broad distinction between two types of scheduling:

  * Queueing. This is appropriate for serial or batch jobs like print spooling
    and requests from a server. There are two main ways of giving priority
    to the jobs in a queue. One is a first-come first-served (FCFS) basis, also
    referred to as first-in first-out (FIFO); the other is to process the shortest
    job first (SJF).

  * Round-robin. This is the time-sharing approach in which several tasks
    can coexist. The scheduler gives a short time-slice to each job, before
    moving on to the next job, polling each task round and round. This way, 
    all the tasks advance, little by little, on a controlled basis.

These two categories are also referred to as non-preemptive and preemptive
respectively, but there is a grey area.

  * Strictly non-preemptive Each program continues executing until it has
    finished, or until it must wait for an event (e.g. I/O or another task). This
    is like Windows 95 and MacIntosh system 7.

  * Strictly preemptive The system decides how time is to be shared between
    the tasks, and interrupts each process after its time-slice whether
    it likes it or not. It then executes another program for a fixed time and
    stops, then the next...etc.

  * Politely-preemptive?? The system decides how time is to be shared, 
    but it will not interrupt a program if it is in a critical section. Certain
    sections of a program may be so important that they must be allowed to
    execute from start to finish withoug being interrupted. This is like UNIX
    and Windows NT.

To choose an algorithm for scheduling tasks we have to understand what it
is we are trying to achieve. i.e. What are the criterea for scheduling?

  * We want to maximize the efficiency of the machine. i.e. we would like all
    the resources of the machine to be doing useful work all of the time - i.e.
    not be idling during one process, when another process could be using
    them. The key to organizing the resources is to get the CPU time-sharing
    right, since this is the central 'organ' in any computer, through which
    almost everything must happen. But this cannot be achieved without
    also thinking about how the I/O devices must be shared, since the I/O
    devices communicate by interrupting the CPU from what it is doing.
    (Most workstations spend most of their time idling. There are enormous
    amounts of untapped CPU power going to waste all over the world each day.)

  * We would like as many jobs to get finished as quickly as possible.

  * Interactive users get irritated if the performance of the machine seems
    slow. We would like the machine to appear fast for interactive users - or
    have a fast response time.

Some of these criterea cannot be met simultaneously and we must make 
compromises. In particular, what is good for batch jobs is often not good for
interactive processes and vice-versa, as we remark under Run levels - priority
below.


4.1.3 Scheduling hierarchy

Complex scheduling algorithms distinguish between short-term and long-term
scheduling. This helps to deal with tasks which fall into two kinds: those which
are active continuously and must therefore be serviced regularly, and those
which sleep for long periods. 

For example, in UNIX the long term scheduler moves processes which have
been sleeping for more than a certain time out of memory and onto disk, to make
space for those which are active. Sleeping jobs are moved back into memory
only when they wake up (for whatever reason). This is called swapping.

The most complex system have several levels of scheduling and exercise
different scheduling polices for processes with different priorities. Jobs can
even move from level to level if the circumstances change.

Figure 4.1: Multi-level scheduling
High	System processes
	Interactive user process
	Batch scheduling
Low	Mostly sleeping processes


4.1.4 Runs levels - priority

Rather than giving all programs equal shares of CPU time, most systems have
priorities. Processes with higher priorities are either serviced more often than
processes with lower priorities, or they get longer time-slices of the CPU.

Priorities are not normally fixed but vary according to the performance of
the system and the amount of CPU time a process has already used up in the 
recent past. For example, processes which have used a lot of CPU time in the 
recent past often have their priority reduced. This tends to favour interative
processes which wait often I/O and makes the response time of the system 
seem faster for interactive users.

In addition, processes may be reduced in priority if their total accumulated
CPU usage becomes very large. (This occurs, for example in UNIX). The 
wisdom of this approach is arguable, since programs which take a long time
to complete tend to be penalized. Indeed, they take must longer to complete
because their priority is reduced. If the priority continued to be lowered, long
jobs would never get finished. This is called process starvation and must be avoided.

Scheduling algorithms have to work without knowing how long processes
will take. Often the best judge of how demanding a program will be is the user
who started the program. UNIX allows users to reduce the priority of
a program themselves using the nice command. 'Nice' users are supposed to
sacrifice their own self-interest for the good of others. Only the system manager
can increase the priority of a process.

Another possibility which is often not considered, is that of increasing the 
priority of resource-gobbling programs in order to get them out of the way as
fast as possible. This is very difficult for an algorithm to judge, so it must be
done manually by the system administrator.


4.1.5 Context switching

Switching from one running process to another running process incurs a cost to
the system. The values of all the registers must be saved in the present state,
the status of all open files must be recorded and the present position in the
program must be recorded. Then the contents of the MMU must be stored
for the process (see next chapter). Then all those things must be read in for
the next process, so that the state of the system is exactly as it was when the
scheduler last interrupted the process. This is called a context switch. Context
switching is a system overhead. It costs real time and CPU cycles, so we don't
want to context switch too often, or a lot of time will be wasted. 

The state of each process is saved to a data structure in the kernel called a
process control block (PCB). 


4.1.6 Interprocess communication 

One of the benefits of multitasking is that processes can be made to
cooperate in order to achieve their ends. To do this, they must do one of the
following:

  * Communicate. Interprocess communication (IPC) involves sending information
    from one process to another. This can be achieved using a 'mailbox'
    system, a socket (Berkeley) which behaves like a virtual communications
    network (loopback), or through the use of 'pipes'. Pipes are a system
    construction which enables one process to open another process as if
    it were a file for writing or reading

  * Share data. A segment of memory must be available to both processes.
    (Most memory is locked to a single process).

  * Waiting. Some processes wait for other processes to give a signal before
    continuing. This is an issue of synchronization.

As soon as we open the door to co-operation there is a problem of how to
synchronize cooperating processes. For example, suppose two processes modify
the same file. If both processes tried to write simultaneously the result would
be a nonsensical mixture. We must have a way of synchronizing processes, so
that even concurrent processes must stand in line to access shared data serially.

Synchronization is a tricky problem in multiprocessor systems, but it can
be achieved with the help of critical sections and semaphores/locks. We shall
return to these below.


4.2 Creation and scheduling


4.2.1 Creating processes

The creation of a process requires the following steps. The order in which they
are carried out is not necessarily the same in all cases.

  1.Name. The name of the program which is to run as the new process 
    must be known.

  2.Process ID and Process Control Block. The system creates a new
    process control block, or locates an unsed block in an array. This block
    is used to follow the execution of the program through its course, keeping
    track of its resources and priority. Each process control block is labelled
    by its PID or process identifier.

  3.Locate the program to be executed on disk and allocate memory for
    the code segment in RAM.

  4.Load the program into the code segment and initialize the registers of the 
    PCB with the start address of the program and appropriate starting 
    values for resources.

  5.Priority. A priority must be computed for the process, using a default
    for the type of process and any value which the user specified as a 'nice'
    value (see Run levels - priorities above).

  6.Schedule the process for execution.


4.2.2 Process hierarchy: children and parent processes

In a demoncratic system anyone can choose to start a new process, but it is never
users which create processes but other processes! That is because anyone using 
the system must already be running a shell or command interpreter in order to
be able to talk to the system, and the command interpreter is itself a process.

When a user creates a process using the command interpreter, the new
process become a child of the command interpreter. Similarly the command
interpreter process becomes the parent for the child. Processes therefore form
a hierarchy.

The processes are linked by a tree structure. If a parent is signalled or killed,
usually all its children receive the same signal or are destoryed with the parent.
This doesn't have to be the case--it is possible to detach children from their 
parent--but in many cases it is useful for processes to be linked in this way.

When a child is created it may do one of two things.

  * Duplicate the parent process.
  * Load a completely new program.

Similarly the parent may do one of two thing.

  * Continue executing along side its children.
  * Wait for some or all of its children to finish before proceeding.


4.2.3 Unix: fork() and wait()

As an example of process creation, we shall consider UNIX. The following example
program is written in C++ and makes use of the standard library function
fork(). The syntax of fork is

returncode = fork();

When this instruction is executed, the process concerned splits into two and
both continue to execute independently from after the fork instruction. If fork
is successful, it returns 0 to the child process and the process identifier or pid
of the child process to the parent. It, for some reason, a new process cannot be
created it returns a value of -1 to the parent.


4.2.4 Process states

In order to know when to execute a program and when not to execute a program,
it is convenient for the scheduler to label programs with a 'state' variable. This
is just an integer value which saves the scheduler time in deciding what to do
with a process. Broadly speaking the state of a process may be one of the following:

  1. New.
  2. Ready (in line to be executed).
  3. Running (active).
  4. Waiting (sleeping, suspended)
  5. Terminated (defunct)

When time-sharing, the scheduler only needs to consider the processes which
are in the 'ready' state. Changes of state are made by the system and follow the 
pattern in the diagram below. The transitions between different states normally
happen on interrupts.

Figure 4.3: Process state diagram.
	New
	
	Running	Running		CPU #1
				CPU #2
				CPU #3
	Waiting	Waiting		
				Terminated


4.2.5 Queue scheduling

The basis of all scheduling is the queue structure. A round-robin sheduler uses
a queue but moves cyclically through the queue at its own speed, instead of
waiting for each task in the queue to complete. Queue scheduling is primarily
used for serial execution.

There are two main types of queue.

  * First-come first-server (FCFS), also called first-in first-out (FIFO).
  * Sorted queue, in which the elements are regularly ordered according to
    some rule. The most prevalent example of this is the shortest job first
    (SJF) rule.

The FCFS queue is the simplest and incurs almost no system overhead. The
SJF scheme can cost quite a lot in system overhead, since each task in the queue
must be evaluated to determine which is shortest. The SJF strategy is often
used for print schedulers since it is quite inexpensive to determine the size of a
file to be printed (the file size is usually stored in the file itself).

The efficiency of the two schemes is subjective: long jobs have to wait longer
if short jobs are moved in front of them, but if the distribution of jobs is random
then we can show that average waiting time of any one job is shorter in
the SJF scheme, because the greatest number of jobs will always be executed
in the shortest possible time.

Of course this argument is rather stupid, since it is only the system which
cares about the average waiting time per job, for its own prestige. Users who
print only long jobs do not share the same clinical viewpoint. Moreover, if only
short jobs arrive after one long job, it is possible that the long job will never
get printed.


4.2.6 Round-robin scheduling

The use of the I/O - CPU burst cycle to requeue jobs improves the resource
utilization considerably, but it does not prevent certain jobs from hogging the 
CPU. Indeed, if one process went into an infinite loop, the whole system would 
stop dead. Also, it does not provide any easy way of giving some processes
priority over others.

A better solution is to ration the CPU time, by introducing time-slices. This 
means that

  1. no process can hold onto the CPU forever,
  2. processes which get requeued often (because they spend a lot of time
     waiting for devices) come around faster, i.e. we don't have to wait for
     CPU intensive processes, and
  3. the length of the time-slices can be varied so as to give priority to 
     particular processes.

The time-sharing is implemented by a hardware timer. On each context
switch, the system loads the timer with the duration of its time-slice and hands
control over to the new process. When the timer times-out, it interrupts the
CPU which then steps in and switches to the next process.

The basic queue is the FCFS/FIFO queue. New processes are added to the 
end, as are processes which are waiting.

The success or failure of round-robin (RR) scheduling depends on the length
of the time-slice or time-quantum. If the slices are too short, the cost of context
switching becomes high in comparision to the time spent doing useful work. If
they become too long, processes which are waiting spend too much time doing
nothing - and in the worst case, everything reverts back to FCFS. A rule of 
thumb is to make the time-slices large enough so that only, say, twenty percent
of all context switches are due to timeouts - the remainder occur freely because
of waiting for requested I/O.


4.2.7 CPU quotas and accounting

Many multiuser systems allow restrictions to be placed on user activity. For
example, it is possible to limit the CPU time used by any one job. If a job exceeds
the limit, it is terminated by the kernel. In order to make such a decision, the
kernel has to keep detailed information about the cumulative use of resources
for each process. This is called accounting and it can be a considerable system
overhead. Most system administrators would prefer not to use accounting -
though unfortunately many are driven to it by thoughtless or hostile users.


4.3 Theads

4.3.1. Heavy and lightweight processes

Threads, sometimes called lightweight processes (LWPs) are indepedently
scheduled parts of a single program. We say that a task is multithreaded if it is
composed of several independent subprocesses which do work on common data, 
and if each of those pieces could (at least in principle) run in parallel.

If we write a program which use threads - there is only one program, one
executable file, one task in the normal sense. Threads simply enable us to
split up that program into logically separate pieces, and have the pieces run
indenpendently of one another, until they need to communicate. In a sense,
threads are a further level of object orientation for multitasking systems. They
allow certain functions to be executed in parallel with others.

On a truly parallel computer (several CPUs) we might imagine parts of a 
program (different subroutines) running on quite different processors, until they
need to communicate. When one part of the program needs to send data to
the other part, the two independent pieces must be synchronized, or be made
to wait for one another. But what is the point of this? We can always run
independent procedures in a program as separate programs, using the process 
mechanisms we have already introduced. They could communicate using normal
interprocesses communication. Why introduce another new concept? Why
do we need threads?

The point is that threads are cheaper than normal processes, and that they
can be scheduled for execution in a user-dependent way, with less overhead.
Threads are cheaper than a whole process because they do not have a full set
of resources each. Whereas the process control block for a heavyweight process
is large and costly to context switch, the PCBs for threads are much smaller,
since each thread has only a stack and some registers to manage. It has no
open file lists or resource lists, no accounting structures to update. All of these
resources are shared by all threads within the process. Threads can be assigned
priorities - a higher priority thread will get put to the front of the queue.

	In other words, threads are processes within processes!
	Threads can only run inside a normal process.

Let's define heavy and lightweight processes with the help of a table.

Object			Resources
Thread (LWP)		Stack + set of CPU registers + CPU time.
Task (HWP)		1 thread + process control block, program code, memory segment etc.
Multithreaded task	n-threads + process control block, program code, memory segment etc.


4.3.2 Why use threads?

From our discussion of scheduling, we can see that the sharing of resources
could have been made more effective if the scheduler had known exactly what
each program was going to do in advance. Of course, the scheduling algorithm
can never know this - but the programmer who wrote the program does know.
Using threads it is possible to organize the execution of a program in such a
way that something is always being done, when ever the scheduler gives the 
heavyweight process CPU time.

  * Threads allow a programmer to switch between lightweight processes
    when it is best for the program. (The programmer has control.)

  * A process which uses threads does not get more CPU time than an 
    ordinary process - but the CPU time it gets is used to do work on the threads.
    It is possible to write a more efficient program by making use of threads.

  * Inside a heavyweight process, threads are scheduled on a FCFS basis, 
    unless the program decides to force certain threads to wait for other threads.
    If there is only one CPU, then only one thread can be running at a time.

  * Threads context switch without any need to involve the kernel - the
    switching is performed by a user level library, so time is saved because
    the kernel doesn't need to know about the threads.


4.3.3 Levels of threads

In modern operating systems, there are two levels at which threads operate:
system or kernel threads and user level threads. If the kernel itself is 
multi-threaded, the scheduler assigns CPU time on a thread basis rather than on a
process basis. A kernel level thread behaves like a virtual CPU, or a powerpoint 
to which user-processes can connect in order to get computing power.
The kernel has as many system level threads as it has CPUs and each of these
must be shared between all of the user-threads on the system. In other words,
the maximum number of user level threads which can be active at any one time
is equal to the number of system level threads, which is turn is equal to the 
number of CPUs on the system.

Since threads work "inside" a single task, the normal process scheduler
cannot normally tell which thread to run and which not to run - that is up
to the program. When hte kernel schedules a process for execution, it must
then find out from that process which is the next thread it must execute. If
the program is lucky enough to have more than one processor available, then
several threads threads can be scheduled at the same time.


4.3.4 Symmetric and asymmetric multiprocessing

Threads are of obvious importance in connection with parallel processing. There
are two approaches to scheduling on a multiprocessor machine:

  * Asymmetric: one CPU does the work of the system, the other CPUs
    service user requests.

  * Symmetric: All processors can be used by the system and users alike.
    No CPU is special.

The asymmetric variant is potentially more wasteful, since it is rare that the 
system requires a whole CPU just to itself. This approach is more common on
very large machines with many processors, where the jobs the system has to do
is quite difficult and warrants a CPU to itself.


4.3.5 Example: POSIX pthreads

The POSIX standardization organization has developed a standard set of function
calls for use of user-level threads. This library is called the pthread interface. 


4.4 Synchronization of processes and threads

When two or more processes work on the same data simultaneously strange
things can happen. We have already seen one example in the threaded file
reader in previous section: when two parallel threads attempt to update the
same variable simultaneously, the result is unpredictable. The value of the
variable afterwards depedns on which of the two threads was the last one to
change the value. This is called a race condition. The value depends on which
of the threads wins the race to update the variable.

What we need in a multitasking system is a way of making such situations
predictable. This is called serialization.


4.4.1 Problem with sharing for processes

It is only threads which need to be synchronized. Suppose one user is
running a script program and editing the program simultaneously. The script
is read in line by line. During the execution of the script, the user adds four
line to the beginning of the file and saves the file. Suddenly, when the next 
line of the executing script gets read, the pointer to the next line points to the
wroing location and it reads in the same line it already read in four lines ago!
Everything in the program is suddenly shifted by four lines, without the process
execting the script knowing about it.

This example (which can actually happen in the UNIX shell) may or may
not return out to be serious - clearly, in general, it can be quite catastrophic. It
is a problem of synchronization on the part of the user and the filesystem.

We must consider programs which share data.

1.When do we need to prevent programs from accessing data simultaneously?
  If there are 100 processes which want to read from a file, this will cause
  not problems because the data themselves are not changed by a read operation.
  A problem only arises if more than one of the parties wants to modify the data.

2.It is even sensible for two programs to want to modify data simultaneously?
  Or is it simply a stupid thing to do? We must be clear about whether
  such collisions can be avoided, or whether they are a necessary part of a
  program. For instance, if two independent processes want to add entries
  to a database, this is a reasonable thing to do. If two unrelated processes
  want to write a log of their activities to the same file, it is probably not 
  sensible: a better solution would be to use two separate files.

3.How should we handle a collision between processes? Should we signal an
  error, or try to make the processes wait in turn? There is no universal 
  answer to this question - in some cases it might be logically incorrect for
  two processes to change data at the same time: if two processes try to
  change one numerical value then one of them has to win - which one?
  On the other hand, if two processes try to add something to a list, that
  make sense, but we have to be sure that they do not write their data on
  top of each other. The writing must happen serially, not in parallel.


4.4.2 Serialization

The key idea in process synchronization is serialization. This means that we
have to go to some pains to undo the work we have put into making an operating
system perform several tasks in parallel. As we mentioned, in the case of print
queues, parallelism is not always appropriate.

Synchronization is a large and difficult topic, so we shall only undertake to
describe the problem and some of the principles involved here.

There are essentially two strategies to serializing processes in a multitasking
environment.

  * The scheduler can be disabled for a short period of time, to prevent control
    being given to another process during a critical action like modifying
    shared data. This method is very inefficient on multiprocessor machines,
    since all other processors have to be halted every time one wishes to
    execute a critical section.

  * A protocol can be introduced which all programs sharing data must obey.
    The protocol ensures that processes have to queue up to gain access to
    shared data. Processes which ignore the protocol ignore it at their own
    peril (and the peril of the remainder of the system!). This method works
    on multiprocessor machine also, though it is more difficult to visualize.

The responsibility of serializing important operations falls on programmers.
The OS cannot impose any restrictions on silly behaviour - it can only provide
tools and mechanisms to assist the solution of the problem.


4.4.3 Mutexes: mutual exclusion 

Another way of talking about serialization is to use the concept of mutual
exclusion. We are interested in allowing only one process or thread access to
shared data at any given time. To serialize access to these shared data, we
have to exclude all processes except for one. Suppose two processes A and B
are trying to access shared data, then: if A is modifying the data, B must be
excluded from doing so; if B modifying the data, A must be excluded from 
doing so. This is called mutual exclusion.

Mutual exclusion can be achieved by a system of locks. A mutual exclusion
lock is colloquially called a mutex. You can see an example of mutex locking
in the multithreaded file reader in the previous section. The idea is for each
thread or process to try to obtain locked-access to shared data:

Get_Mutex(m);

// Update shared data

Release_Mutex(m);

The mutex variable is shared by all parties (e.g. a global variable). This
protocol is meant to ensure that only one process at a time can get past the
function Get_mutex. All other processes or threads are made to wait at the 
function Get_mutex until that one process calls Release_Mutex to release the
lock. A method for implementing this is discussed below. Mutexes are a central
part of multithreaded programming.


4.4.4 User synchronization: file locks

A simple example of a protocol solution, to the locking problem at the user
level, is the so-called file-lock in UNIX. When write-access is required to a file,
we try to obtain a lock by creating a lock-file with a special name. If another
user or obtain a lock by creating a lock-file with a special name. If another
user or process has already obtained a lock, then the file is already in use, and
we are denied permission to edit the file. If the file is free, a 'lock' is placed on
the file by creating the file lock. This indicates that the file now belongs to the 
new user. When the user has finished, the file lock is deleted, allowing others
to use the file.

In most cases a lock is simply a text file. If we wanted to edit a file blurb,
the lock might be called blurb.lock and contain the user identifier of the user
currently editing the file. If other users then try to access the file, they find
that the lock file exists and are denied access. When the user is finished with
the file, the lock is removed.

The same method of locks can also be used to prevent two instances of a
program from starting up simultaneously. This is often used in mail programs
such as the ELM mailer in UNIX, since it would be unwise to try to read and
delete incoming mail with two instances of the mail program at the same time.


4.4.5 Exclusive and non-exclusive locks

To control both read and write access to files, we can use a system of exclusive
and not-exclusive locks.

If a user wishes to read a file, a non-exclusive lock is used. Other users
can also get non-exclusive locks to read the file simultaneously, but when a 
non-exclusive lock is placed on a file, no user may write to it.

To write to a file, we must get an exclusive lock. When an exclusive lock is
obtained, no other users can read or write to the file.


4.4.6 Critical sections: the mutex solution

A critical section is a part of a program in which is it necessary to have exclusive
access to shared data. Only one process or thread may be in a critical section at
any one time.

In the past it was possible to implement this is by generalizing the idea of imterrupt
masks, as mentioned in chapter 2. By switching off interrupts (or more appropriately,
by switching off the scheduler) a process can guarantee itself uninterrupted 
access to shared data. This method has drawbacks: (i) masking interrupts
can be dangerous - there is always the possibility that important
interrupts will be missed, (ii) it is not general enough in a multiprocessor 
environment, since interrupts will continue to be serviced by other processors -
so all processors would have to be switched off; (iii) it is too harsh. We only need
to prevent two programs from being in their critical sections simultaneously if
they share the same data. Programs A and B might share differnt data to programs 
C and D, so why should they wait for C and D?

The modern way of implementing a critical section is to use mutexes as we
have described above. In 1981 G.L. Peterson discovered a simple algorithm for
achieving mutual exclusion between two processes with PID equal to 0 or 1.


4.4.7 Flags and semaphores

Flags are similar in concept to locks. The idea is that two cooperating processes
can synchronize their execution by sending very simple messages to each other.
A typical behaviour is that one process decides to stop and wait until another
process signals that it has arrived at a certain place.

A semaphore is a flag which can have a more general value than just true of false. A
semaphore is an integer counting variable and is used to solve problems where
there is competition between processes. The idea is that one part of a program 
tends to increment the semaphore while another part tends to decrement the
semaphore. The value of the flag variable dictates whether a program will wait
or continue, or thether something special will occur. There are many uses for
semaphores and we shall not go into them here. A simple example is reading
and writing via buffers, where we count how many items are in the buffer. 
When the buffer becomes full, the process which is filling it must be made to 
wait until space in the buffer is made available.


4.4.8 Monitors

Some languages (like Modula) have special language class-environments for
dealing with mutual exclusion. Such an environment is called a monitor.

  * A monitor is a language-device which removes some of the pain from
    synchronization. Only one process can be 'inside' a monitor at a time
    - users don't need to code this themselves, they only have to create a
    monitor.

  * A precedure or function defined under the umbrella of a monitor can only
    access those shared memory locations declared within that monitor and
    vice-versa.

  * Wait and signal operations can be defined to wait for specific condition
    variables. A process can thus wait until another process sends a signal or
    semaphore which changes the condition variable.


4.5 Deadlock

Waiting and synchronization is not all sweetness and roses. Consider the European
road rule which says: on minor roads one should always wait for traffic
coming from the right. If your cars arrive simultaneously at a crossroads
then, according to the rule all of them must wait for each other and none
of them can ever move. This situation is called deadlock. It is the stale-mate of
the operating system world.


4.5.1 Cause

Deadlock occurs when a number of processes are waiting for an event which
can only be caused by another of the waiting processes.

These are the essential requirements for a deadlock:

1.Circular waiting. There must be a set of processes P1..Pn where P1 is
  waiting for a resource or signal from P2, P2 is waiting for P3 ... and Pn is 
  waiting for P1.

2.Non-sharable resources. It is not possible to share the resources or signals
  which are being waited for. If the resource can be shared, there is no 
  reason to wait. 

3.No preemption. The processes can not be forced to give up the resources 
  they are holding.

There are likewise three methods for handling deadlock situations:

1.Prevention. We can try to design a protocol which ensures that deadlock
  never occurs.

2.Recovery. We can allow the system to enter a deadlock state and then recover.

3.Ostrich method. we can pretend that deadlocks will never occur and live
  happily in our ignorance. This is the method used by most operating
  systems. User programs are expected to behave properly. The system
  does not interface. This is understandable: it is very hard to make general
  rules for every situation which might arise.


4.5.2 Prevention

Deadlock prevention requires a system overhead.

The simplest possibility for avoidance of deadlock is to introduce an extra
layer of software for requesting resources in addition to a certain amount of
accounting. Each time a new request is made, the system analyses the allocation
of resources before granting or refusing the resource. The same applies for wait
conditions.

The problem with this approach is that, if a process is not permitted to
wait for another process - what should it do instead? At best the system would
have to reject or terminate programs which could enter deadlock, returning an
error condition.

Another method is the following. One might demand that all programs 
declare what resources they will need in advance. Similarly all wait conditions
should be declared. The system could then analyse (and re-analyse each time
a new process arrives) the resource allocation and pin-point possible problems.


4.5.3 Detection

The detection of deadlock conditions is also a system overhead. At regular intervals
the system is required to examine the state of all processes and determine the
interrelations between them. Since this is quite a performance burden, it
is not surprising that most systems ignore deadlocks and expect users to write
careful programs.


4.5.4 Recovery

To recover from a deadlock, the system must either terminate one of the participants,
and go on terminating them until the deadlock is cured, or repossess the 
resources which are causing the deadlock from some processes until the deadlock
is cured. The latter method is somewhat dangerous since it can lead to
incorrect program execution. Processes usually wait for a good reason, and any
interruption of that reasoning could lead to incorrect execution. Termination 
is a safer alternative.


4.6 Summary

In this chapter we have considered the creation and scheduling of processes.
Each process may be described by

  * A process identifier.
  * A process control block which contains status information about the 
    scheduled processes.
  * A private stack for that process.

The scheduling of processes takes place by a variety of emthods. The aim is to
maximize the use of CPU time and spread the load for the devices.

Processes can be synchronized using semaphores or flags. Protocol constructions
such as critical sections and monitors guarantee that shared data are not
modified by more than one process at a time.

If a process has to wait for a condition which can never arise until it has 
finished waiting, then a deadlock is said to arise. The cause of deadlock waiting
is often a resource which cannot be shared. Most operating systems do not try 
to prevent deadlocks, but leave the problem to user programs.

