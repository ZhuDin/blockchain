Introduction

When I was 12, my brother and I studied piano. Each week we would make a trip
to our teacher’s house; while one of us had our lesson, the other would wait in
her parlor. Fortunately, she always had a few games arranged on a coffee table to
help us pass the time while waiting. One game I remember consisted of a series of
pegs on a small piece of wood. Little did I know it, but the game would prove to
be an early introduction to data structures and algorithms.

The game was played as follows. All of the pegs were white, except for one,
which was blue. To begin, one of the white pegs was removed to create an empty
hole. Then, by jumping pegs and removing them much like in checkers, the game
continued until a single peg was left, or the remaining pegs were scattered about
the board in such a way that no more jumps could be made. The object of the
game was to jump pegs so that the blue peg would end up as the last peg and in
the center. According to the game’s legend, this qualified the player as a “genius.”
Additional levels of intellect were prescribed for other outcomes. As for me, I felt
satisfied just getting through a game without our teacher’s kitten, Clara, pouncing
unexpectedly from around the sofa to sink her claws into my right shoe. I suppose 
being satisfied with this outcome indicated that I simply possessed “common
sense.”

I remember playing the game thinking that certainly a deterministic approach
could be found to get the blue peg to end up in the center every time. What I was
looking for was an algorithm. Algorithms are well-defined procedures for solving
problems. It was not until a number of years later that I actually implemented an
algorithm for solving the peg problem. I decided to solve it in LISP during an 
artificial intelligence class in college. To solve the problem, I represented information
about the game in various data structures. Data structures are conceptual organizations 
of information. They go hand in hand with algorithms because many algorithms 
rely on them for efficiency.

Often, people deal with information in fairly loose forms, such as pegs on a board,
notes in a notebook, or drawings in a portfolio. However, to process information
with a computer, the information needs to be more formally organized. In addition, 
it is helpful to have a precise plan for exactly what to do with it. Data structures 
and algorithms help us with this. Simply stated, they help us develop
programs that are, in a word, elegant. As developers of software, it is important to
remember that we must be more than just proficient with programming languages
and development tools; developing elegant software is a matter of craftsmanship.
A good understanding of data structures and algorithms is an important part of
becoming such a craftsman.


An Introduction to Data Structures

Data comes in all shapes and sizes, but often it can be organized in the same way.
For example, consider a list of things to do, a list of ingredients in a recipe, or a
reading list for a class. Although each contains a different type of data, they all
contain data organized in a similar way: a list. A list is one simple example of a
data structure. Of course, there are many other common ways to organize data as
well. In computing, some of the most common organizations are linked lists,
stacks, queues, sets, hash tables, trees, heaps, priority queues, and graphs, all of
which are discussed in this book. Three reasons for using data structures are efficiency, 
abstraction, and reusability.

Efficiency
	Data structures organize data in ways that make algorithms more efficient. For
	example, consider some of the ways we can organize data for searching it.
	One simplistic approach is to place the data in an array and search the data by
	traversing element by element until the desired element is found. However,
	this method is inefficient because in many cases we end up traversing every
	element. By using another type of data structure, such as a hash table (see
	Chapter 8, Hash Tables) or a binary tree (see Chapter 9, Trees) we can search
	the data considerably faster.

Abstraction
	Data structures provide a more understandable way to look at data; thus, they
	offer a level of abstraction in solving problems. For example, by storing data
	in a stack (see Chapter 6, Stacks and Queues), we can focus on things that we
	do with stacks, such as pushing and popping elements, rather than the details
	of how to implement each operation. In other words, data structures let us
	talk about programs in a less programmatic way.

Reusability
	Data structures are reusable because they tend to be modular and context-free.
	They are modular because each has a prescribed interface through which
	access to data stored in the data structure is restricted. That is, we access the
	data using only those operations the interface defines. Data structures are
	context-free because they can be used with any type of data and in a variety
	of situations or contexts. In C, we make a data structure store data of any type
	by using void pointers to the data rather than by maintaining private copies of
	the data in the data structure itself.

When one thinks of data structures, one normally thinks of certain actions, or
operations, one would like to perform with them as well. For example, with a list,
we might naturally like to insert, remove, traverse, and count elements. A data
structure together with basic operations like these is called an abstract datatype.
The operations of an abstract datatype constitute its public interface. The public
interface of an abstract datatype defines exactly what we are allowed to do with it.
Establishing and adhering to an abstract datatype’s interface is essential because
this lets us better manage a program’s data, which inevitably makes a program
more understandable and maintainable.


An Introduction to Algorithms

Algorithms are well-defined procedures for solving problems. In computing, algorithms 
are essential because they serve as the systematic procedures that computers 
require. A good algorithm is like using the right tool in a workshop. It does the
job with the right amount of effort. Using the wrong algorithm or one that is not
clearly defined is like cutting a piece of paper with a table saw, or trying to cut a
piece of plywood with a pair of scissors: although the job may get done, you have
to wonder how effective you were in completing it. As with data structures, three
reasons for using formal algorithms are efficiency, abstraction, and reusability.

Efficiency
	Because certain types of problems occur often in computing, researchers have
	found efficient ways of solving them over time. For example, imagine trying to
	sort a number of entries in an index for a book. Since sorting is a common
	task that is performed often, it is not surprising that there are many efficient
	algorithms for doing this. We explore some of these in Chapter 12, Sorting
	and Searching.

Abstraction
	Algorithms provide a level of abstraction in solving problems because many
	seemingly complicated problems can be distilled into simpler ones for which
	well-known algorithms exist. Once we see a more complicated problem in a
	simpler light, we can think of the simpler problem as just an abstraction of the
	more complicated one. For example, imagine trying to find the shortest way
	to route a packet between two gateways in an internet. Once we realize that
	this problem is just a variation of the more general single-pair shortest-paths
	problem (see Chapter 16, Graph Algorithms), we can approach it in terms of this
	generalization.

Reusability
	Algorithms are often reusable in many different situations. Since many wellknown 
	algorithms solve problems that are generalizations of more complicated 
	ones, and since many complicated problems can be distilled into
	simpler ones, an efficient means of solving certain simpler problems potentially 
	lets us solve many others.


General Approaches in Algorithm Design

In a broad sense, many algorithms approach problems in the same way. Thus, it is
often convenient to classify them based on the approach they employ. One reason 
to classify algorithms in this way is that often we can gain some insight about
an algorithm if we understand its general approach. This can also give us ideas
about how to look at similar problems for which we do not know algorithms. Of
course, some algorithms defy classification, whereas others are based on a combination 
of approaches. This section presents some common approaches.

Randomized algorithms

Randomized algorithms rely on the statistical properties of random numbers. One
example of a randomized algorithm is quicksort (see Chapter 12).

Quicksort works as follows. Imagine sorting a pile of canceled checks by hand.
We begin with an unsorted pile that we partition in two. In one pile we place all
checks numbered less than or equal to what we think may be the median value,
and in the other pile we place the checks numbered greater than this. Once we
have the two piles, we divide each of them in the same manner and repeat the
process until we end up with one check in every pile. At this point the checks are
sorted.

In order to achieve good performance, quicksort relies on the fact that each time
we partition the checks, we end up with two partitions that are nearly equal in
size. To accomplish this, ideally we need to look up the median value of the
check numbers before partitioning the checks. However, since determining the
median requires scanning all of the checks, we do not do this. Instead, we randomly 
select a check around which to partition. Quicksort performs well on
average because the normal distribution of random numbers leads to relatively
balanced partitioning overall.

Divide-and-conquer algorithms

Divide-and-conquer algorithms revolve around three steps: divide, conquer, and
combine. In the divide step, we divide the data into smaller, more manageable
pieces. In the conquer step, we process each division by performing some operation 
on it. In the combine step, we recombine the processed divisions. One example 
of a divide-and-conquer algorithm is merge sort (see Chapter 12).

Merge sort works as follows. As before, imagine sorting a pile of canceled checks
by hand. We begin with an unsorted pile that we divide in half. Next, we divide
each of the resulting two piles in half and continue this process until we end up
with one check in every pile. Once all piles contain a single check, we merge the
piles two by two so that each new pile is a sorted combination of the two that
were merged. Merging continues until we end up with one big pile again, at
which point the checks are sorted.

In terms of the three steps common to all divide-and-conquer algorithms, merge
sort can be described as follows. First, in the divide step, divide the data in half.
Next, in the conquer step, sort the two divisions by recursively applying merge
sort to them. Last, in the combine step, merge the two divisions into a single
sorted set.

Dynamic-programming solutions

Dynamic-programming solutions are similar to divide-and-conquer methods in that
both solve problems by breaking larger problems into subproblems whose results
are later recombined. However, the approaches differ in how subproblems are
related. In divide-and-conquer algorithms, each subproblem is independent of the
others. Therefore, we solve each subproblem using recursion (see Chapter 3,
Recursion) and combine its result with the results of other subproblems. In
dynamic-programming solutions, subproblems are not independent of one
another. In other words, subproblems may share subproblems. In problems like
this, a dynamic-programming solution is better than a divide-and-conquer
approach because the latter approach will do more work than necessary, as shared
subproblems are solved more than once. Although it is an important technique
used by many algorithms, none of the algorithms in this book use dynamic
programming.

Greedy algorithms

Greedy algorithms make decisions that look best at the moment. In other words,
they make decisions that are locally optimal in the hope that they will lead to
globally optimal solutions. Unfortunately, decisions that look best at the moment
are not always the best in the long run. Therefore, greedy algorithms do not
always produce optimal results; however, in some cases they do. One example of
a greedy algorithm is Huffman coding, which is an algorithm for data compression 
(see Chapter 14, Data Compression).

The most significant part of Huffman coding is building a Huffman tree. To build a
Huffman tree, we proceed from its leaf nodes upward. We begin by placing each
symbol to compress and the number of times it occurs in the data (its frequency)
in the root node of its own binary tree (see Chapter 9). Next, we merge the two
trees whose root nodes have the smallest frequencies and store the sum of the 
frequencies in the new tree’s root. We then repeat this process until we end up with
a single tree, which is the final Huffman tree. The root node of this tree contains
the total number of symbols in the data, and its leaf nodes contain the original
symbols and their frequencies. Huffman coding is greedy because it continually
seeks out the two trees that appear to be the best to merge at any given time.

Approximation algorithms

Approximation algorithms are algorithms that do not compute optimal solutions;
instead, they compute solutions that are “good enough.” Often we use approximation 
algorithms to solve problems that are computationally expensive but are too
significant to give up on altogether. The traveling-salesman problem (see
Chapter 16) is one example of a problem usually solved using an approximation
algorithm.

Imagine a salesman who needs to visit a number of cities as part of the route he
works. The goal in the traveling-salesman problem is to find the shortest route
possible by which the salesman can visit every city exactly once before returning
to the point at which he starts. Since an optimal solution to the traveling-salesman
problem is possible but computationally expensive, we use a heuristic to come up
with an approximate solution. A heuristic is a less than optimal strategy that we
are willing to accept when an optimal strategy is not feasible.

The traveling-salesman problem can be represented graphically by depicting the
cities the salesman must visit as points on a grid. We then look for the shortest
tour of the points by applying the following heuristic. Begin with a tour consisting
of only the point at which the salesman starts. Color this point black. All other
points are white until added to the tour, at which time they are colored black as
well. Next, for each point v not already in the tour, compute the distance between
the last point u added to the tour and v. Using this, select the point closest to u,
color it black, and add it to the tour. Repeat this process until all points have been
colored black. Lastly, add the starting point to the tour again, thus making the tour
complete.


A Bit About Software Engineering

As mentioned at the start of this chapter, a good understanding of data structures
and algorithms is an important part of developing well-crafted software. Equally
important is a dedication to applying sound practices in software engineering in
our implementations. Software engineering is a broad subject, but a great deal can
be gleaned from a few concepts, which are presented here and applied throughout 
the examples in this book.

Modularity
	One way to achieve modularity in software design is to focus on the development 
	of black boxes. In software, a black box is a module whose internals are
	not intended to be seen by users of the module. Users interact with the module 
	only through a prescribed interface made public by its creator. That is, the
	creator publicizes only what users need to know to use the module and hides
	the details about everything else. Consequently, users are not concerned with
	the details of how the module is implemented and are prevented (at least in
	policy, depending on the language) from working with the module’s internals. 
	These ideas are fundamental to data hiding and encapsulation, principles 
	of good software engineering enforced particularly well by objectoriented 
	languages. Although languages that are not object-oriented do not
	enforce these ideas to the same degree, we can still apply them. One example
	in this book is the design of abstract datatypes. Fundamentally, each datatype
	is a structure. Exactly what one can do with the structure is dictated by the
	operations defined for the datatype and publicized in its header.

Readability
	We can make programs more readable in a number of ways. Writing meaningful 
	comments, using aptly named identifiers, and creating code that is 
	selfdocumenting are a few examples. Opinions on how to write good comments
	vary considerably, but a good fundamental philosophy is to document a
	program so that other developers can follow its logic simply by reading its
	comments. On the other hand, sections of self-documenting code require few,
	if any, comments because the code reads nearly the same as what might be
	stated in the comments themselves. One example of self-documenting code in
	this book is the use of header files as a means of defining and documenting
	public interfaces to the data structures and algorithms presented.

Simplicity
	Unfortunately, as a society we tend to regard “complex” and “intelligent” as
	words that go together. In actuality, intelligent solutions are often the simplest
	ones. Furthermore, it is the simplest solutions that are often the hardest to
	find. Most of the algorithms in this book are good examples of the power of
	simplicity. Although many of the algorithms were developed and proven
	correct by individuals doing extensive research, they appear in their final form
	as clear and concise solutions to problems distilled down to their essence.

Consistency
	One of the best things we can do in Software development is to establish coding 
	conventions and stick to them. Of course, conventions must also be easy
	to recognize. After all, a convention is really no convention at all if someone
	else is not able to determine what the convention is. Conventions can exist on
	many levels. For example, they may be cosmetic, or they may be more related
	to how to approach certain types of problems conceptually. Whatever the
	case, the wonderful thing about a good convention is that once we see it in
	one place, most likely we will recognize it and understand its application
	when we see it again. Thus, consistency fosters readability and simplicity as
	well. Two examples of cosmetic conventions in this book are the way comments 
	are written and the way operations associated with data structures are
	named. Two examples of conceptual conventions are the way data is
	managed in data structures and the way static functions are used for private
	functions, that is, functions that are not part of public interfaces.


How to Use This Book

This book was designed to be read either as a textbook or a reference, whichever
is needed at the moment. It is organized into three parts. The first part consists of
introductory material and includes chapters on pointer manipulation, recursion,
and the analysis of algorithms. These subjects are useful when working in the rest
of the book. The second part presents fundamental data structures, including
linked lists, stacks, queues, sets, hash tables, trees, heaps, priority queues, and
graphs. The third part presents common algorithms for solving problems in sorting, 
searching, numerical analysis, data compression, data encryption, graph theory, 
and computational geometry.

Each of the chapters in the second and third parts of the book has a consistent 
format to foster the book’s ease of use as a reference and its readability in general.
Each chapter begins with a brief introduction followed by a list of specific topics
and a list of real applications. The presentation of each data structure or algorithm
begins with a description, followed by an interface, followed by an implementation 
and analysis. For many data structures and algorithms, examples are presented 
as well. Each chapter ends with a series of questions and answers, and a
list of related topics for further exploration.

The presentation of each data structure or algorithm starts broadly and works
toward an implementation in real code. Thus, readers can easily work up to the
level of detail desired. The descriptions cover how the data structures or algorithms 
work in general. The interfaces serve as quick references for how to use the
data structures or algorithms in a program. The implementations and analyses 
provide more detail about exactly how the interfaces are implemented and how each
implementation performs. The questions and answers, as well as the related topics, 
help those reading the book as a textbook gain more insight about each chapter. 
The material at the start of each chapter helps clearly identify topics within the
chapters and their use in real applications.